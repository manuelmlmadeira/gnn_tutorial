{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Tutorial: Graph Neural Networks with Pytorch\n",
        "\n",
        "By: [Manuel Madeira](https://manuelmlmadeira.github.io/)\n",
        "\n",
        "Adapted from [EPFL EE-452 - Network Machine Learning](https://edu.epfl.ch/coursebook/en/network-machine-learning-EE-452)\n",
        "\n",
        "## Instructions\n",
        "\n",
        "We will have **coding** and **theoretical** questions. \n",
        "\n",
        "- Coding exercises should be solved within the specified space:\n",
        "    ```python\n",
        "    # Your solution here ###########################################################\n",
        "    ...\n",
        "    #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "    ```\n",
        "    Sometimes we provide variable names, such as `x = ...`; do not change names and stick to hinted typing, as they will be reused later.\n",
        "\n",
        "- Theoretical questions are set in markdown cells and can be identified by the following pattern:\n",
        "    ```markdown\n",
        "    **Your answer:**\n",
        "    ...\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmp4-YzspA2V"
      },
      "source": [
        "## Objective\n",
        "\n",
        "This tutorial focuses on Graph Neural Networks.\n",
        "In the first part, you will load and prepare data using the PyTorch Geometric library, a well-known library for graph deep learning.\n",
        "Then we will implement a GNN, implement train and test functions, and comment on the results.\n",
        "In the second part, you will define a new GNN architecture and include it in the previous architecture.\n",
        "\n",
        "‚ö†Ô∏è PyTorch/PyTorch Geometric have no CUDA support to run on Apple Silicon Macs (M1/M2/M3/M4) yet. An alternative is to install those packages from source, but given the time constraints, we recommend directly using a linux platform to run this tutorial (e.g., Google Colab). \n",
        "\n",
        "‚ö†Ô∏è Make sure that the Python version in your environment is not greater than 3.11.\n",
        "\n",
        "Run the cell below to install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt\n",
        "# We will need to install two additional libraries beyond the base installation of PyTorch Geometric: pyg_lib and torch-sparse. \n",
        "%pip install pyg_lib torch_sparse -f https://data.pyg.org/whl/torch-2.4.0+cu118.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation\n",
        "import torch\n",
        "\n",
        "TORCH = torch.__version__.split('+')[0]\n",
        "CUDA = torch.__version__.split('+')[1]\n",
        "print(f\"PyTorch version: {TORCH}\")\n",
        "print(f\"CUDA version: {CUDA}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch_geometric as pyg\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from torch_geometric.nn.models import GCN, GAT, GIN\n",
        "from torchmetrics.classification import BinaryF1Score\n",
        "from scipy import sparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv2QsxmdLWNN"
      },
      "source": [
        "## Section 0: Explore the data\n",
        "\n",
        "In this section, we will go through the data to get a feeling of its content. We work with the [GitHub dataset](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.GitHub.html), from the [Multi-scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) paper. In this dataset, .\n",
        "\n",
        "Important information about the dataset:\n",
        "- Nodes represent developers on GitHub and edges are mutual follower relationships\n",
        "- The node features correspond to an embedding of location, starred repositories, employer, and e-mail address information of each user.\n",
        "- The node labels indicates whether it corresponds to a web or a machine learning developer.\n",
        "- The full dataset contains 37 700 nodes, 578 006 edges, 128-dimensional node features and 2 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-ZfCASHUyyK"
      },
      "outputs": [],
      "source": [
        "# Set the device to GPU if available. Otherwise, use CPU.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# We now load the data and preprocess it. We will also create a mask that is True for the training set instances (80% of the data) and False for the test set ones.\n",
        "data, train_mask = utils.fetch_and_preprocess_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An important aspect of graph datasets in PyTorch Geometric is that they leverage the sparse of real graphs to store and operate over them in a more efficient way. \n",
        "\n",
        "For example, for a graph with 37 700 nodes as ours, its adjacency matrix has shape 37 700 √ó 37 700 matrix, which has approximately 1 421 290 000 entries. Suppose that this graph is stored with a **dense** representation with 1 bit per entry, then this would result in ~170 MB of storage space. If 8 bits (1 byte) are used per entry, then this would give us ~1.42 GB. Since PyTorch uses 64-bit floats (8 bytes) per entry, we would get more than 10 GB for this graph. This illustrates that even though many entries may be zero (meaning no edge) in this 37 700 x 37 700 matrix, a dense representation allocates space for all possible node pairs. However, in a **sparse** representation, instead of storing all possible entries, only the existing edges (nonzero entries) are stored, which can significantly reduce the required space (if you do the math it's ~2.2 MB in this example) üòÑ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task:\n",
        "\n",
        "We define now the task we intend to solve: predict the label of unseen nodes in the social network graph. \n",
        "\n",
        "Note that, to define the training set, we use a masking strategy instead of directly partitioning the graph because our interpretation of the task is that we have a social network in which the training labels are accessible, while, for the test nodes, we can access their embeddings but not their labels. In practice, this simplifies the sampling strategy, in particular for network methods, as we do not have to worry about loosing structure.\n",
        "\n",
        "Also, importantly, as the dataset is unbalanced, we will consider as a representative metric of performance the **F1 score**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TTeSCNpAhNL"
      },
      "source": [
        "## Section 1: Machine and Deep learning baselines on graph data\n",
        "\n",
        "We will start with classical ML baselines, to get some results to which we can compare. \n",
        "\n",
        "We will provide all the functions for the steps in this section as they are not the focus of this tutorial. Nevertheless, you should follow the steps below to understand to where we are heading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kyFm7htUGtx"
      },
      "source": [
        "### Baseline 1: Random Forest Classifier based on node features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = data.x[train_mask].numpy()\n",
        "y_train = data.y[train_mask].numpy()\n",
        "x_test = data.x[~train_mask].numpy()\n",
        "y_test = data.y[~train_mask].numpy()\n",
        "rf_node_feats_f1 = utils.rf_train_and_test(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
        "print(f\"Random Forest F1 score: {rf_node_feats_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxBcITtgLRXQ"
      },
      "source": [
        "### Baselines 2: Graph structure-based approach via Laplacian eigendecomposition\n",
        "\n",
        "Now, let's implement a second benchmark, this time relying on some graph structural properties. \n",
        "\n",
        "An important concept in graph theory is the **Laplacian** of a graph. The Laplacian matrix is defined as:\n",
        "\n",
        "$$\n",
        "L = D - A\n",
        "$$\n",
        "\n",
        "where $D$ is the degree matrix (a diagonal matrix where each entry $d_{ii}$ is the degree of node $i$) and $A$ is the adjacency matrix.\n",
        "\n",
        "We skip the details here, but the takeaway message is that the eigenvalues and eigenvectors of the Laplacian matrix are representative of the graph structure. Thus, we will use them to inform our baseline of the graph structure.\n",
        "\n",
        "Note that this requires computing the eigen decomposition of the Laplacian matrix. This is a very expensive operation, and we will not be able to do it for our graph with 37 700 nodes - we would quickly run out of memory! (Go ahead and try if you wish üòâ)\n",
        "\n",
        "Nevertheless, the adjacency matrix (and, thus, the graph Laplacian) is mainly filled with zeros. So, we can optimize memory and running time by using a **sparse representation** as discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUkPozOLXsU2"
      },
      "outputs": [],
      "source": [
        "RUN_EIGENDECOMPOSITION = True  # Set to True to run the eigendecomposition of the Laplacian matrix (will take ~5-10 min). You can leave as False to skip this step and use random vectors instead.\n",
        "\n",
        "# Eigendecomposition of the Laplacian matrix\n",
        "A = sparse.coo_array(\n",
        "    (\n",
        "        np.ones(data.edge_index.shape[1]),\n",
        "        (data.edge_index[0].numpy(), data.edge_index[1].numpy()),\n",
        "    )\n",
        ")\n",
        "D = sparse.diags(A.sum(0))\n",
        "laplacian = D - A\n",
        "\n",
        "if RUN_EIGENDECOMPOSITION: \n",
        "    _eigvals, eigvecs = sparse.linalg.eigsh(laplacian, k=6, which=\"SM\")\n",
        "    eigvecs = eigvecs[:, 1:]\n",
        "else:\n",
        "    num_nodes = data.x.shape[0]\n",
        "    eigvecs = np.random.rand(num_nodes, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will consider now a baseline that only considers the graph structure (only takes the eigendecomposition information into consideration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9auF0OKHsQ6",
        "outputId": "c0fa19e4-cc95-44e2-c0b9-51b1da87269b"
      },
      "outputs": [],
      "source": [
        "# Only eigenvectors\n",
        "x_train = eigvecs[train_mask]\n",
        "y_train = data.y[train_mask].numpy()\n",
        "x_test = eigvecs[~train_mask]\n",
        "y_test = data.y[~train_mask].numpy()\n",
        "\n",
        "rf_eigvecs_f1 = utils.rf_train_and_test(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
        "print(f\"Random Forest with only structure F1 score: {rf_eigvecs_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline 3: Structure-aware baseline using node features and graph structure\n",
        "\n",
        "Now we merge both previous approaches: we will concatenate the node features and graph Laplacian eigenvectors to \n",
        "\n",
        "graph structure Merging both the node features and the structure (basically, merging both previous baselines by concatenating their input features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eigenvectors + node features\n",
        "cat_feats = np.concatenate([data.x.numpy(), eigvecs], axis=1)\n",
        "x_train = cat_feats[train_mask]\n",
        "y_train = data.y[train_mask].numpy()\n",
        "x_test = cat_feats[~train_mask]\n",
        "y_test = data.y[~train_mask].numpy()\n",
        "rf_cat_feats_f1 = utils.rf_train_and_test(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
        "print(f\"Random Forest with structure and node features F1 score: {rf_cat_feats_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRXIUrGNWpKe"
      },
      "source": [
        "### Baseline 4: Deep Learning baseline - MLP\n",
        "\n",
        "In this question, we move from classical ML to Deep Learning and, again, we start from a simple model to get a viable benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH3G5rTBtp6D"
      },
      "outputs": [],
      "source": [
        "# Dataset objects\n",
        "batch_size = 128\n",
        "loader_train = DataLoader(\n",
        "    TensorDataset(data.x[train_mask], data.y[train_mask]),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "loader_test = DataLoader(\n",
        "    TensorDataset(data.x[~train_mask], data.y[~train_mask]),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "# Model\n",
        "mlp = utils.MLP(data.x.shape[1], hidden_features=64).to(device)\n",
        "\n",
        "# Training objects\n",
        "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=0.005)\n",
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG1gxH1xDBw0"
      },
      "source": [
        "We now proceed to training our model. We gather the losses of each batch, and plot the evolution of the training loss at the end. Finally, we compute the F1 score on the training and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_nn = utils.train_nn(loader_train=loader_train,model=mlp,loss_fn=loss_fn,optimizer=optimizer,n_epochs=n_epochs,device=device)\n",
        "\n",
        "# Evaluation\n",
        "metric_fn = BinaryF1Score().to(device)\n",
        "mlp_train_f1 = utils.eval_nn(model=mlp, loader=loader_train, metric_fn=metric_fn, device=device)\n",
        "mlp_test_f1 = utils.eval_nn(model=mlp, loader=loader_test, metric_fn=metric_fn, device=device)\n",
        "print(f\"MLP F1 score in training set: {mlp_train_f1:.3f}\")\n",
        "print(f\"MLP F1 Score in test set:     {mlp_test_f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1_scores = {\n",
        "    \"Random Forest w/ Node Features\": rf_node_feats_f1,\n",
        "    \"Random Forest w/ Structure\": rf_eigvecs_f1,\n",
        "    \"Random Forest w/ Node Features + Structure\": rf_cat_feats_f1,\n",
        "    \"MLP w/ Node Features\": mlp_test_f1,\n",
        "}\n",
        "\n",
        "\n",
        "def plot_f1_scores(f1_scores_dict):\n",
        "    # Sort scores\n",
        "    sorted_items = sorted(f1_scores_dict.items(), key=lambda x: x[1])\n",
        "    models, scores = zip(*sorted_items)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    bars = plt.barh(models, scores, color='skyblue')\n",
        "    plt.xlabel(\"F1 Score\")\n",
        "    plt.title(\"F1 Score by Model\")\n",
        "    plt.xlim(0, 1)\n",
        "\n",
        "    # Add score labels to bars\n",
        "    for bar in bars:\n",
        "        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "                f\"{bar.get_width():.2f}\", va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcX8v2A0OlhQ"
      },
      "source": [
        "Important observations:\n",
        "- Including structure to classic ML approaches marginally improves the performance.\n",
        "- The deep learning-based model leads to the best performance!\n",
        "\n",
        "Can we go even further?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Graph Neural Networks\n",
        "\n",
        "We now shift from the standard deep learning paradigm to Graph Neural Networks to additionally leverage the structure of our data. \n",
        "\n",
        "It's now your time to shine! Let's start by defining our first GNN. \n",
        "\n",
        "It will be a subclass of the PyTorch `Module`, but this time it will take into account the `edge_index` in its `forward method`. \n",
        "The `edge_index` consists of a 2xE matrix, where E is the number of edges in the graph. The first row contains the source nodes and the second row contains the target nodes. For undirected graphs, the edge index is symmetric, meaning that if there is an edge from node i to node j, there is also an edge from node j to node i.\n",
        "\n",
        "Build a GCN with two layers to go from input features, here called *channels*, to a hidden dimension defined in the constructor, then to logit readout. Use ReLU activations. This GNN will map node vectors to node logits.\n",
        "\n",
        "*Hint:* you can have a look at the GCN update rule [here](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyGCN(nn.Module):\n",
        "    def __init__(self, in_channels: int, hidden_channels: int):\n",
        "        super().__init__()\n",
        "        # Your solution here #######################################################\n",
        "        self.linear1 = nn.Linear(in_channels, hidden_channels)\n",
        "        self.linear2 = nn.Linear(hidden_channels, 1)\n",
        "        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Your solution here #######################################################\n",
        "        # You should convert the edge_index to an adjacency matrix A, and then compute the normalized adjacency matrix A_norm.\n",
        "        num_nodes = x.size(0)\n",
        "        row, col = edge_index\n",
        "\n",
        "        # Build adjacency matrix A\n",
        "        A = torch.zeros((num_nodes, num_nodes), device=x.device)\n",
        "        A[row, col] = 1\n",
        "        A += torch.eye(num_nodes, device=x.device)  # Add self-loops\n",
        "\n",
        "        # Compute D^{-1/2}\n",
        "        D = A.sum(dim=1)\n",
        "        D_inv_sqrt = torch.diag(torch.pow(D, -0.5))\n",
        "\n",
        "        # Compute normalized adjacency matrix\n",
        "        A_norm = D_inv_sqrt @ A @ D_inv_sqrt\n",
        "\n",
        "        # First GCN layer\n",
        "        x = A_norm @ x\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second GCN layer\n",
        "        x = A_norm @ x\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x\n",
        "        # Note: it should output logits!\n",
        "        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "\n",
        "# Initialize a model from the class you just defined.\n",
        "myGCN = MyGCN(in_channels=data.x.shape[1], hidden_channels=64).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also instantiate a model from the built-in GCN implementation from PyTorch Geometric, so that you can have a ground truth to compare with your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pygGCN = GCN(in_channels=data.x.shape[1], hidden_channels=64, num_layers=2, out_channels=1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwZEvTJlHbKb"
      },
      "source": [
        "Now write the code to perform `n_epochs` (already defined above) epochs of training with the GCN models above, using full training data as a batch. Make sure to only use training data in the loss computation by using the `train_mask`. Track the loss value at each step and plot it. Finally, evaluate the model on train and test, using the `metric_fn` from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "pllLi-s9H79q",
        "outputId": "137a031d-2973-43a6-b63d-41f976b714da"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_gnn_full(model, data, loss_fn, optimizer, n_epochs, device):\n",
        "    loss_list = []\n",
        "    # Your solution here ###########################################################\n",
        "    data = data.to(device)\n",
        "    for epoch in tqdm(range(n_epochs), desc=\"Training epochs\"):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(data.x, data.edge_index).sigmoid().squeeze()\n",
        "        loss = loss_fn(y_pred[train_mask], data.y[train_mask].double())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.item())\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "def evaluate_gnn(model, data, train_mask, metric_fn):\n",
        "    # Your solution here ###########################################################\n",
        "    y_pred = model(data.x, data.edge_index).squeeze()\n",
        "    metric_fn.reset()\n",
        "    metric_tr = metric_fn(y_pred[train_mask], data.y[train_mask])\n",
        "    metric_fn.reset()\n",
        "    metric_te = metric_fn(y_pred[~train_mask], data.y[~train_mask])\n",
        "    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "    return metric_tr, metric_te\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define some helper functions to launch the training and testing process, as well as to plot the training loss against training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss_list(loss_list):\n",
        "    plt.plot(range(1, n_epochs + 1), loss_list, marker='o')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Average Loss\")\n",
        "    plt.title(\"Training Loss per Epoch\")\n",
        "    plt.show()\n",
        "\n",
        "def train_and_test_gnn(gnn_model, data, n_epochs):\n",
        "    loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "    optimizer = optim.Adam(gnn_model.parameters(), lr=0.01)\n",
        "    loss_list = train_gnn_full(gnn_model, data, loss_fn, optimizer, n_epochs, device)\n",
        "    plot_loss_list(loss_list)\n",
        "    metric_fn = BinaryF1Score().to(device)\n",
        "    train_f1, test_f1 = evaluate_gnn(pygGCN, data, train_mask, metric_fn)\n",
        "    print(f\"{gnn_model.__class__.__name__} Training F1-score: {train_f1:.3f}\")\n",
        "    print(f\"{gnn_model.__class__.__name__} Test F1-Score:     {test_f1:.3f}\")\n",
        "    return train_f1.item(), test_f1.item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first test the groundtruth model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs = 10\n",
        "pyg_GCN_train_f1, pyg_GCN_test_f1 = train_and_test_gnn(pygGCN, data, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now we test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "myGCN_train_f1, myGCN_test_f1 = train_and_test_gnn(myGCN, data, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Did you see any difference between both implementations? If so, what do you think is the reason for that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer:**\n",
        "Performance wise both models are similar, but the PyG implementation of the GCN is much faster than our implementation. This is because PyG uses a sparse representation of the graph, which is much more efficient than the dense representation we used in our implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceMxKgYzOmGI"
      },
      "source": [
        "We got already some good results!\n",
        "\n",
        "As you noticed, our previous implementation employed a full gradient step. However, it could be benefitial to use mini-batched training of our model for two main reasons\n",
        "- GPU usage: for larger graphs, they may not fill in a single GPU;\n",
        "- Stochastic optimization might yield better results.\n",
        "\n",
        "But could we design such a setting?\n",
        "\n",
        "Batching large graph data requires a careful approach, since on top of the design matrix with node features we have to account for edge information. In our setting, we have a single graph with many nodes, and a node level task. A batching strategy consists in sampling nodes with their neighbors, then working with this smaller graphs in a batched way. Again, PyTorch Geometric has a built-in operator for this: we define one [NeighborLoader](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborLoader) for our data, which will gather neighbors for as many *iterations* as layers in your GCN. Check [mini batches](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#mini-batches) if needed.\n",
        "\n",
        "We illustrate how to proceed in this case below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbtiFv4a4aPW"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "loader_graph_batches = pyg.loader.NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[-1] * 2,  # Use all neighbors for both layers (we could also use only a subset)\n",
        "    input_nodes=train_mask,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "def train_and_test_gnn_batched(gnn_model, batched_loader, n_epochs):\n",
        "    loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "    optimizer = optim.Adam(gnn_model.parameters(), lr=0.01)\n",
        "    loss_list = utils.train_gnn_batched(batched_loader, gnn_model, loss_fn, optimizer, n_epochs, device)\n",
        "    plot_loss_list(loss_list)\n",
        "    metric_fn = BinaryF1Score().to(device)\n",
        "    train_f1, test_f1 = evaluate_gnn(pygGCN, data, train_mask, metric_fn)  # evaluation can be done on the full graph\n",
        "    print(f\"{gnn_model.__class__.__name__} Training F1-score: {train_f1:.3f}\")\n",
        "    print(f\"{gnn_model.__class__.__name__} Test F1-Score:     {test_f1:.3f}\")\n",
        "    return train_f1.item(), test_f1.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pyg_GCN_batched_train_f1, pyg_GCN_batched_test_f1 = train_and_test_gnn_batched(\n",
        "    pygGCN,\n",
        "    loader_graph_batches,\n",
        "    n_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GNN architecures beyond GCN\n",
        "\n",
        "In this section, we will explore some of the GNN architectures available in PyTorch Geometric. \n",
        "\n",
        "Check the list of models available [here](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#models). Pick two of them, instantiate them and observe their results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1\n",
        "# Your solution here #######################################################\n",
        "pyg_gnn1 = GAT(in_channels=data.x.shape[1], hidden_channels=64, num_layers=2, out_channels=1).to(device)\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "pyg_gnn1_train_f1, pyg_gnn1_test_f1 = train_and_test_gnn_batched(\n",
        "    pyg_gnn1,\n",
        "    loader_graph_batches,\n",
        "    n_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 2\n",
        "# Your solution here #######################################################\n",
        "pyg_gnn2 = GIN(in_channels=data.x.shape[1], hidden_channels=64, num_layers=2, out_channels=1).to(device)\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "pyg_gnn2_train_f1, pyg_gnn2_test_f1 = train_and_test_gnn_batched(\n",
        "    pyg_gnn2,\n",
        "    loader_graph_batches,\n",
        "    n_epochs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Wrapping up\n",
        "\n",
        "Time to compare all models we implemented so far!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f1_scores[\"My GCN\"] = myGCN_test_f1\n",
        "f1_scores[\"PyG GCN\"] = pyg_GCN_test_f1\n",
        "f1_scores[\"PyG GCN Batched\"] = pyg_GCN_batched_test_f1\n",
        "f1_scores[\"PyG GNN 1\"] = pyg_gnn1_test_f1\n",
        "f1_scores[\"PyG GNN 2\"] = pyg_gnn2_test_f1\n",
        "\n",
        "plot_f1_scores(f1_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Your Answer:**\n",
        "Dependent on architecture chosen. Importantly, we did not include any validation curve, so we cannot say if the model is overfitting or not. Please take this into consideration before taking very definitive conclusions. üôÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note for the future: how to proceed in the case of small graphs?\n",
        "\n",
        "We have demonstrated how to proceed in the case of large graphs. \n",
        "\n",
        "However, for many real-world applications (*e.g.* molecular graphs), the considered graphs are rather small (< 100 nodes). In these cases, we can obtain the node label predictions for nodes spanning several entire graphs in a single batch. In this case, we can use directly the `DataLoader` class from PyTorch Geometric to create mini-batches. This class takes care of batching the data for us, and it is very easy to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Other Resources\n",
        "\n",
        "In case you may want to learn more about the tools used in this tutorial, here are some links to the official documentation and tutorials:\n",
        "\n",
        "* [PyTorch: Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
        "* [PyTorch Geometric: Official Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html#official-examples)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "inductiva_gnn4",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
